# ShareSafe AI Text Detection System - Comprehensive Audit

================================================================================
                              DETECTION FLOWCHART
================================================================================

                    +----------------------------------+
                    |       USER OPENS WEB PAGE        |
                    +----------------------------------+
                                    |
                                    v
                    +----------------------------------+
                    |   Content Script Loads           |
                    |   (contentNew.js)                |
                    +----------------------------------+
                                    |
                                    v
                    +----------------------------------+
                    |   Platform Detection             |
                    |   Twitter? Wikipedia? Generic?   |
                    +----------------------------------+
                                    |
                                    v
            +-----------------------------------------------+
            |              SEGMENT EXTRACTION               |
            |  - Find paragraphs, headings, lists           |
            |  - Skip navigation/footer/sidebar             |
            |  - Calculate word count per segment           |
            +-----------------------------------------------+
                                    |
                                    v
                    +----------------------------------+
                    |   For Each Segment:              |
                    +----------------------------------+
                                    |
                    +---------------+---------------+
                    |                               |
                    v                               v
    +---------------------------+     +---------------------------+
    |   STATISTICAL ANALYSIS    |     |   PATTERN DETECTION       |
    |   (statisticalAnalyzer)   |     |   (segmentAnalyzer)       |
    |                           |     |                           |
    |   - Sentence variance     |     |   - AI phrases            |
    |   - Token entropy         |     |   - Markdown artifacts    |
    |   - N-gram repetition     |     |   - Terminal emojis       |
    |   - Lexical diversity     |     |   - Self-disclosure       |
    |   - Transition density    |     |   - Template phrases      |
    +---------------------------+     +---------------------------+
                    |                               |
                    +---------------+---------------+
                                    |
                                    v
                    +----------------------------------+
                    |   SCORE COMBINATION              |
                    |                                  |
                    |   Short Text (<50 words):        |
                    |     40% stats + 60% patterns     |
                    |                                  |
                    |   Long Text (50+ words):         |
                    |     75% stats + 25% patterns     |
                    +----------------------------------+
                                    |
                                    v
                    +----------------------------------+
                    |   Score in Uncertain Zone?       |
                    |   (35-65 range)                  |
                    +----------------------------------+
                           |                |
                          YES               NO
                           |                |
                           v                v
            +---------------------------+   |
            |   LLM TIE-BREAKER         |   |
            |   (Gemini API)            |   |
            |                           |   |
            |   Blend: 60% stat + 40%   |   |
            |   LLM result              |   |
            +---------------------------+   |
                           |                |
                           +-------+--------+
                                   |
                                   v
                    +----------------------------------+
                    |   CONFIDENCE CALCULATION         |
                    |                                  |
                    |   Based on:                      |
                    |   - Feature agreement            |
                    |   - Word count                   |
                    |   - Signal strength              |
                    +----------------------------------+
                                    |
                                    v
                    +----------------------------------+
                    |   RISK LEVEL ASSIGNMENT          |
                    |                                  |
                    |   Score >= 60: HIGH RISK         |
                    |   Score 35-59: MEDIUM RISK       |
                    |   Score < 35: LOW RISK           |
                    +----------------------------------+
                                    |
                                    v
            +-----------------------------------------------+
            |              PAGE AGGREGATION                 |
            |                                               |
            |   Weighted average of all segment scores      |
            |   Weight = (wordCount/100) x (confidence/100) |
            |   Longer + higher confidence = more influence |
            +-----------------------------------------------+
                                    |
                                    v
                    +----------------------------------+
                    |   VISUAL OUTPUT                  |
                    |                                  |
                    |   - Highlight flagged segments   |
                    |   - Show summary panel           |
                    |   - Display reasons              |
                    +----------------------------------+


================================================================================
                    DETAILED EXPLANATION OF EACH STEP
================================================================================

STEP 1: PAGE LOAD & INITIALIZATION
----------------------------------
When a user opens any webpage, the content script (contentNew.js) automatically 
loads. It first checks if the extension is enabled and if the domain is 
allowed. Then it detects the platform type (Twitter, Wikipedia, generic, etc.) 
to apply appropriate detection strategies.


STEP 2: SEGMENT EXTRACTION
--------------------------
The system doesn't analyze the entire page as one block. Instead, it breaks 
the content into logical segments:

  - Paragraphs (<p> elements)
  - Headings with their following content (<h1>-<h6>)
  - List items (<li>)
  - Blockquotes
  - Content divs

Navigation, headers, footers, and sidebars are skipped. Each segment is 
assigned an ID and its word count is calculated.

WHY THIS MATTERS: A page might have human-written intro with AI-generated 
body. Segment-level analysis can detect this mixed content.


STEP 3: STATISTICAL ANALYSIS
----------------------------
For each segment, statistical features are calculated:

  1. SENTENCE METRICS
     - Mean sentence length (AI tends to be consistent ~18-22 words)
     - Sentence length variance (AI has low variance)
     - Burstiness (human writing has irregular rhythm)

  2. VOCABULARY METRICS
     - Token entropy (AI has more predictable word choices)
     - Lexical diversity (AI may repeat vocabulary)
     - Stopword ratio (function word patterns)

  3. STYLE METRICS
     - Transition density ("moreover", "furthermore" overuse)
     - Filler phrase ratio ("it is important to note")
     - POS regularity (grammar pattern consistency)

Each feature is normalized using Z-score against human baseline values.
Features are weighted, and weights are REDUCED for short text where 
statistics are unreliable.


STEP 4: PATTERN DETECTION
-------------------------
Fast regex-based checks for known AI fingerprints:

  HIGH CERTAINTY PATTERNS (Strong signals):
  - "As an AI language model..."
  - "I don't have personal opinions..."
  - Markdown formatting in casual text (### headers, **bold**)
  - Terminal emoji placement (emoji at sentence end only)
  - "It is important to note that..."

  MEDIUM CERTAINTY PATTERNS:
  - "In conclusion", "To summarize"
  - Corporate jargon ("leverage", "optimize", "holistic")
  - Rhetorical questions with marketing tone
  - Excessive formal connectors

If a HIGH CERTAINTY pattern is detected, it can override statistical scores
(called "Red Flag Override").


STEP 5: SCORE COMBINATION
-------------------------
Statistical and pattern scores are combined differently based on text length:

  FOR SHORT TEXT (<50 words):
    Final Score = (stat_score x 0.4) + (pattern_score x 0.6)
    
    Reasoning: Statistics are unreliable on short text, so patterns 
    (like AI phrases, emojis) are weighted more heavily.

  FOR LONG TEXT (50+ words):
    Final Score = (stat_score x 0.75) + (pattern_score x 0.25)
    
    Reasoning: Statistics become reliable with more data, so they 
    dominate the score.

  RED FLAG OVERRIDE:
    If definitive AI patterns detected, score is forced to minimum 85
    regardless of statistical results.


STEP 6: LLM TIE-BREAKER (OPTIONAL)
----------------------------------
The system can optionally use Google Gemini as a tie-breaker, but ONLY when:

  - User has enabled LLM mode in settings
  - Score is in "uncertain zone" (35-65)
  - Text has at least 20 words

LLM is NEVER called when:
  - Score < 35 (clearly human-like)
  - Score > 65 (clearly AI-like)
  - Text is too short

When used, the final score blends 60% statistical + 40% LLM judgment.
This prevents LLM from overriding strong statistical evidence.


STEP 7: CONFIDENCE CALCULATION
------------------------------
Confidence (0-100) indicates how reliable the score is:

  HIGH CONFIDENCE when:
  - Multiple features agree
  - Text is long (100+ words)
  - Strong patterns detected

  LOW CONFIDENCE when:
  - Features disagree
  - Text is short
  - No strong patterns
  - Score is in uncertain zone

Confidence is capped at 40% for very short text (<50 words) to prevent
overconfident predictions on insufficient data.


STEP 8: RISK LEVEL ASSIGNMENT
-----------------------------
Each segment is bucketed into risk levels:

  HIGH RISK:    Score >= 60 (55 for short text with adaptive threshold)
  MEDIUM RISK:  Score 35-59
  LOW RISK:     Score < 35

Segments marked as MEDIUM or HIGH risk are flagged for user attention.


STEP 9: PAGE AGGREGATION
------------------------
All segment scores are combined into a page-level score using weighted average:

  Weight = (segment_word_count / 100) x (segment_confidence / 100)

This means:
  - Longer segments have more influence
  - Higher confidence segments have more influence
  - A short low-confidence AI segment won't dominate the page score

The page is then assigned an overall risk level based on:
  - Number of high-risk segments
  - Number of medium-risk segments
  - Weighted average score


STEP 10: VISUAL OUTPUT
----------------------
The user sees:

  1. SUMMARY PANEL (top-right corner):
     - Count of HIGH / MEDIUM / LOW risk segments
     - Total segments analyzed
     - "Hide Highlights" button

  2. SEGMENT HIGHLIGHTS:
     - Colored border around flagged text
     - Badge showing score percentage
     - Click for detailed tooltip with reasons

  3. TOOLTIP DETAILS:
     - Score and confidence
     - Top 5 reasons why AI was suspected
     - Whether LLM was used


================================================================================
                        HOW THE DETECTION ACTUALLY WORKS
================================================================================

THE CORE IDEA:
--------------
AI-generated text has measurable stylistic patterns that differ from natural
human writing. The system detects these patterns through:

  1. STATISTICAL FINGERPRINTS
     AI tends to write with consistent sentence lengths, predictable vocabulary,
     and regular grammatical structures. Humans are more chaotic.

  2. PHRASE FINGERPRINTS
     AI uses certain phrases frequently: "It is important to note", 
     "In conclusion", "Let's dive in". These are rare in organic human writing.

  3. FORMATTING FINGERPRINTS
     Copy-pasted AI text often contains markdown (### headers, **bold**),
     numbered lists, or terminal emojis that look unnatural in casual context.


THE DETECTION FORMULA (simplified):
-----------------------------------

  IF text_length < 50 words:
      ai_score = (0.4 * statistical_score) + (0.6 * pattern_score)
      IF definitive_pattern_found:
          ai_score = max(ai_score, 85)
      confidence = min(calculated_confidence, 40)  # Capped for short text
  
  ELSE:
      ai_score = (0.75 * statistical_score) + (0.25 * pattern_score)
      confidence = calculated_confidence
  
  IF ai_score >= 60:
      risk_level = "HIGH"
  ELIF ai_score >= 35:
      risk_level = "MEDIUM"
  ELSE:
      risk_level = "LOW"


WHY PARTIAL DETECTION IS HONEST:
--------------------------------
Unlike binary "AI or Human" classifiers, this system:

  - Scores each segment independently (0-100)
  - Shows WHERE in the document AI is suspected
  - Explains WHY with specific reasons
  - Admits uncertainty with confidence scores
  - Never says "definitively AI" - always "suspected" or "likely"

This is safer because:
  - Human-edited AI text is correctly shown as "mixed"
  - Formal human writing (Wikipedia) may flag but with low confidence
  - Users can make informed decisions with the evidence provided


================================================================================


## 1. Existing Detection Factors

### Statistical Features (statisticalAnalyzer.js)

| Factor | What It Measures | Why It Signals AI | Strength |
|--------|------------------|-------------------|----------|
| Sentence Length Mean | Average words per sentence | AI tends toward consistent ~18-22 word sentences | Weak |
| Sentence Length Variance | Variation in sentence length | AI produces uniform sentence structures | Medium |
| Token Entropy | Shannon entropy of word distribution | Low entropy = predictable/templated vocabulary | Medium |
| N-gram Repetition | Repeated 3-word phrases | AI reuses phrase patterns more than humans | Medium |
| Readability Grade | Flesch-Kincaid complexity | AI often optimizes for "readable" grade 8-12 | Weak |
| POS Regularity | Part-of-speech pattern consistency | AI has more regular grammatical patterns | Medium |
| Lexical Diversity (TTR) | Unique words / total words | Low diversity = limited vocabulary variation | Medium |
| Burstiness | Consecutive sentence length changes | Humans have irregular "bursts"; AI is smooth | Weak |
| Stopword Ratio | Function words per total words | AI often has consistent function word usage | Weak |
| Transition Density | "moreover", "however" per sentence | AI overuses formal connectors | Strong |
| Filler Ratio | "It is important to note" etc. | Direct AI phrase fingerprinting | Strong |
| Emoji Placement | Terminal vs. mid-sentence emojis | AI puts emojis at sentence end only | Strong |
| Copy-Paste Artifacts | ###, **bold**, numbered lists | Ghost markdown from LLM copy-paste | Strong |

### Pattern-Based Heuristics (segmentAnalyzer.js)

| Factor | What It Detects | Strength |
|--------|-----------------|----------|
| Explicit AI Mentions | "As an AI", "ChatGPT wrote this" | Definitive |
| AI Self-Identification | "I don't have personal opinions" | Definitive |
| Template Phrases | "It's worth noting", "In conclusion" | Strong |
| Excessive Formal Connectors | 3+ uses of "moreover/furthermore" | Medium |
| Corporate Jargon | "comprehensive", "holistic", "robust" | Medium |
| Hedging Language | "can be seen as", "might be" | Medium |
| Citation-less Claims | "Studies show..." without source | Medium |
| Terminal Emoji Pattern | Emoji at exact sentence end | Strong |
| Markdown Artifacts | Bold/headers in casual text | Strong |
| Numbered Lists | "1. Plan 2. Execute" in short text | Medium |


## 2. Performance by Text Length

| Content Type | Expected Accuracy | Strengths | Weaknesses |
|--------------|-------------------|-----------|------------|
| Long articles (500+ words) | 70-80% | Stats stabilize; N-grams emerge | May miss edited AI; flags journalism |
| Medium blogs (150-500 words) | 60-75% | Good balance of stats + patterns | Wikipedia-style triggers FPs |
| Short text (<50 words) | 50-65% | Red flags effective | Stats unreliable; confidence capped |
| Social media posts | 45-60% | Copy-paste artifacts visible | Very short; hashtags confuse stats |


## 3. Known Limitations

1. SHORT TEXT UNCERTAINTY
   Problem: Stats need ~100+ words to stabilize.
   Why Hard: 15-word tweet has 1-2 sentences - not enough data.

2. WIKIPEDIA / ENCYCLOPEDIC WRITING
   Problem: Encyclopedic writing triggers AI signals.
   Why Hard: AI was literally trained on Wikipedia - they share the same style.

3. EDITED / PARAPHRASED AI TEXT
   Problem: Light editing breaks phrase fingerprints.
   Why Hard: Changing "It is" to "It's" defeats detection while keeping AI ideas.

4. NON-NATIVE ENGLISH BIAS
   Problem: ESL writers may use simpler, consistent structures.
   Why Hard: System compares against "native human writing" baselines.

5. ADVERSARIAL HUMANIZATION
   Problem: Users can add typos, contractions, questions.
   Why Hard: Every signal is learnable and counterable.


## 4. Final Verdict

### Is this system reliable?

YES, with caveats:
- RELIABLE for flagging potential AI influence
- Segment-level is fairer than page-level
- Explainable reasons build trust
- NOT reliable for short text (<50 words)
- WILL flag Wikipedia/formal writing
- Should NOT be used for academic integrity alone

### Best Suited For

| User Type | Fit |
|-----------|-----|
| Journalists verifying sources | Excellent |
| Content moderators | Good (with human review) |
| Curious readers | Good |
| Academic integrity officers | Insufficient alone |
| Legal evidence | Not appropriate |

### User Communication

"This tool identifies patterns commonly associated with AI-generated text. 
It does not prove authorship. A high score means 'worth reviewing,' not 
'definitely AI.' False positives occur on formal, encyclopedic, or edited 
content. Use as one signal among many."

================================================================================
SECTION TITLE
================================================================================
Pattern Detection Logic â€” Detailed Technical Breakdown

================================================================================
OBJECTIVE
================================================================================
Document the internal logic, detection rules, and reasoning for EACH pattern-based 
AI signal used in ShareSafe.

These pattern detectors are part of `segmentAnalyzer.js` and are used to identify 
AI-style writing fingerprints.

================================================================================
PATTERNS TO DOCUMENT
================================================================================

1. AI Phrases
--------------------------------------------------------------------------------
Examples: "It is important to note", "In conclusion", "This article explores"

1. **Detection Logic:** 
   - Uses a `RegExp` pattern library (`aiPhrases` array) with word boundary anchors (`\b`).
   - Each match adds a specific weight to the segment's pattern score (e.g., "It is important to note" adds +18 points).
   - Some phrases (e.g., "moreover", "therefore") use a frequency threshold (must appear 3+ times) to trigger.

2. **Why AI models trigger this:** 
   - RLHF training biases models toward academic, neutral, and explanatory tones.
   - Models use transitional phrases to structure logic ("In conclusion") or hedge statements ("It is important to note").

3. **Why humans may trigger it (edge cases):** 
   - Academic writing, technical documentation, and formal journalism often use these precise transitions.

4. **How ShareSafe mitigates false positives:** 
   - Context detection: Signals are weighted less in "Long Text" mode where statistics rule.
   - Thresholds: Single occurrences of common connectors (like "moreover") are ignored; only clusters trigger the signal.

5. **Signal strength:** 
   - **Strong** for specific multi-word phrases ("It is important to note").
   - **Weak/Medium** for single transitional words unless heavily repeated.

--------------------------------------------------------------------------------

2. Markdown Artifacts
--------------------------------------------------------------------------------
Examples: **Bold headers**, ### Headings, Numbered lists in casual text

1. **Detection Logic:** 
   - Scans for leftover Markdown syntax usually stripped by front-ends but present in raw LLM output.
   - Regex: `/\*\*(.*?)\*\*/` (bold) and `/\b\d+\.\s+[A-Z]/` (numbered lists).
   - Logic: Triggers ONLY if the text segment is short (<200 characters) AND contains multiple instances.

2. **Why AI models trigger this:** 
   - LLMs default to Markdown formatting for structure. Users often copy-paste the raw "Chat" output directly.

3. **Why humans may trigger it (edge cases):** 
   - Developers writing technical comments.
   - Users deliberately formatting Reddit/Discord posts (though ShareSafe targets web page content where raw markdown is rare).

4. **How ShareSafe mitigates false positives:** 
   - Length Constraint: This check is DISABLED for text > 200 characters (where markdown might be legitimate content structure).
   - Frequency Threshold: Requires >= 2 instances to flag.

5. **Signal strength:** 
   - **Strong / Definitive** for short text comments.

--------------------------------------------------------------------------------

3. Terminal Emojis
--------------------------------------------------------------------------------
Examples: "Hereâ€™s the summary ğŸš€", "Hope this helps ğŸ˜Š"

1. **Detection Logic:** 
   - Regex checks for an emoji character located specifically at the *very end* of the text string.
   - Pattern: `/[\u{1F300}-\u{1FAFF}][^a-zA-Z0-9]*$/u` (Emoji followed by only non-alphanumeric chars at end of string).

2. **Why AI models trigger this:** 
   - Models are trained to be "helpful assistants" and often sign off responses with a single friendly emoji.

3. **Why humans may trigger it (edge cases):** 
   - Social media users ending posts with emojis.
   - Use of single emojis as reactions.

4. **How ShareSafe mitigates false positives:** 
   - Position Specificity: Only flags if the emoji is the *terminal* character. Mid-sentence emojis are ignored or count toward human "burstiness".

5. **Signal strength:** 
   - **Strong** (weighted heavily in "Short Text" mode).

--------------------------------------------------------------------------------

4. Self-Disclosure Patterns
--------------------------------------------------------------------------------
Examples: "As an AI language model", "I donâ€™t have personal opinions"

1. **Detection Logic:** 
   - Regex matching against known safety refusal and identity templates.
   - Patterns: `/\b(as an ai|generated by ai|i do not have personal)\b/i`.

2. **Why AI models trigger this:** 
   - Hardcoded safety alignments and refusal behaviors in base models (OpenAI, Anthropic, Google).

3. **Why humans may trigger it (edge cases):** 
   - Quoting AI responses (e.g., writing an article *about* AI).

4. **How ShareSafe mitigates false positives:** 
   - It doesn't heavily mitigate this, as it's a "Definitve" signal. The text *is* AI text, even if quoted.
   - Detection triggers a "Red Flag Override".

5. **Signal strength:** 
   - **Definitive** (Overrides all other statistics to set score >= 85).

--------------------------------------------------------------------------------

5. Template Phrases
--------------------------------------------------------------------------------
Examples: "Letâ€™s break this down", "Here are some key points", "Structured lists"

1. **Detection Logic:** 
   - Detects structural repetition and formulaic setups.
   - **Repetitive Entrances:** Checks if multiple sentences start with the same words (`uniqueStarts.size < sentences.length * 0.6`).
   - **List Detection:** Regex `/\b\d+\.\s+[A-Z]/g` identifies densely packed numbered lists in paragraphs.

2. **Why AI models trigger this:** 
   - Models optimize for clarity and structure, leading to repetitive sentence structures and frequent usage of list formats.

3. **Why humans may trigger it (edge cases):** 
   - Instructional writing (recipes, tutorials) often uses imperative lists.

4. **How ShareSafe mitigates false positives:** 
   - Ratios: Uses a ratio of unique sentence starts vs total sentences. Occasional repetition is fine; systematic repetition flags.

5. **Signal strength:** 
   - **Medium** (Contributes ~8-10 points to score; rarely triggers detection alone).

============================================================================================================================================================


1ï¸âƒ£ AI PHRASES DETECTION
Code Block
JavaScript

// Located in: detectAIPatterns(text) function

// â”€â”€â”€ Direct AI Mentions â”€â”€â”€
if (/\b(ai[- ]generated|generated by ai|created (by|with) ai)\b/i.test(text)) {
  pushReason('[AI] Explicitly marked as AI-generated');
  addScore(90);
  hasStrongSignal = true;
}

if (/\b(chatgpt|gpt-4|claude|gemini|copilot) (generated|created|wrote|made)\b/i.test(text)) {
  pushReason('[AI] Tool authorship indicated');
  addScore(85);
  hasStrongSignal = true;
}

// â”€â”€â”€ Common AI Phrases â”€â”€â”€
const aiPhrases = [
  { pattern: /\b(it'?s worth noting|it'?s important to note)\b/i, score: 15, msg: '[Style] Common AI transitional phrase' },
  { pattern: /\b(as an ai|as a language model|i don'?t have personal)\b/i, score: 95, msg: '[AI] Self-identification', strong: true },
  { pattern: /\b(in (conclusion|summary|today'?s|this))\b/i, score: 10, msg: '[Style] Formulaic transitions' },
  { pattern: /\b(delve|leverage|utilize|facilitate|enhance|optimize)\b/i, score: 12, msg: '[Vocab] Overuse of formal vocabulary' },
  { pattern: /\b(comprehensive|holistic|robust|seamless|cutting[- ]edge)\b/i, score: 8, msg: '[Vocab] Corporate jargon' },
  { pattern: /\b(it is (important|crucial|essential|vital) to (note|understand|remember))\b/i, score: 18, msg: '[Style] AI emphasis pattern' },
  { pattern: /\b(moreover|furthermore|additionally|consequently|therefore)\b/gi, score: 0, count: true, threshold: 3, scoreMulti: 12, msg: '[Style] Excessive formal connectors' },
  { pattern: /\b(can be (seen|viewed|considered|understood) as)\b/i, score: 10, msg: '[Style] Hedging language' },
  { pattern: /\b(range of|variety of|number of|series of)\b/gi, score: 0, count: true, threshold: 2, scoreMulti: 8, msg: '[Style] Generic quantifiers' },
  { pattern: /\b(plays a (crucial|vital|key|important|significant) role)\b/i, score: 14, msg: '[Style] AI cliche phrase' }
];

aiPhrases.forEach(({ pattern, score: patternScore, msg, strong, count, threshold, scoreMulti }) => {
  if (count) {
    const matches = text.match(pattern);
    if (matches && matches.length >= threshold) {
      pushReason(`${msg} (${matches.length}Ã—)`);
      addScore(scoreMulti * Math.min(matches.length, 5));
    }
  } else {
    if (pattern.test(text)) {
      pushReason(msg);
      addScore(patternScore);
      if (strong) hasStrongSignal = true;
    }
  }
});
Explanation
text

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI PHRASES DETECTION                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WHAT IT DOES:                                                  â”‚
â”‚  Searches for specific phrases that AI commonly uses            â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  HOW IT WORKS:                                                  â”‚
â”‚                                                                 â”‚
â”‚  1. DIRECT AI MENTIONS (Highest Confidence)                     â”‚
â”‚     â”œâ”€â”€ "AI-generated" â†’ +90 score                              â”‚
â”‚     â”œâ”€â”€ "generated by AI" â†’ +90 score                           â”‚
â”‚     â””â”€â”€ "ChatGPT created" â†’ +85 score                           â”‚
â”‚                                                                 â”‚
â”‚  2. AI PHRASES ARRAY (Loop through each)                        â”‚
â”‚     â”œâ”€â”€ Single-match patterns (test once)                       â”‚
â”‚     â”‚   Example: "it's worth noting" â†’ +15 score                â”‚
â”‚     â”‚                                                           â”‚
â”‚     â””â”€â”€ Count-based patterns (count occurrences)                â”‚
â”‚         Example: "moreover" 3+ times â†’ +12 Ã— count              â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  PATTERN STRUCTURE:                                             â”‚
â”‚  {                                                              â”‚
â”‚    pattern: /regex/,     â† What to search for                   â”‚
â”‚    score: 15,            â† Points to add if found               â”‚
â”‚    msg: 'Reason',        â† Explanation for user                 â”‚
â”‚    strong: true,         â† Marks as definitive signal           â”‚
â”‚    count: true,          â† Enable counting mode                 â”‚
â”‚    threshold: 3,         â† Minimum matches needed               â”‚
â”‚    scoreMulti: 12        â† Points per match                     â”‚
â”‚  }                                                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Visual Flow
text

INPUT TEXT: "It's worth noting that this comprehensive guide 
             leverages cutting-edge technology. Moreover, 
             furthermore, additionally, it is important to note..."

DETECTION PROCESS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  Pattern Check                       â”‚ Found? â”‚ Score Added     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  "it's worth noting"                 â”‚ âœ… Yes â”‚ +15             â”‚
â”‚  "comprehensive"                     â”‚ âœ… Yes â”‚ +8              â”‚
â”‚  "leverages"                         â”‚ âœ… Yes â”‚ +12             â”‚
â”‚  "cutting-edge"                      â”‚ âœ… Yes â”‚ +8              â”‚
â”‚  "moreover/furthermore/additionally" â”‚ âœ… 3Ã—  â”‚ +36 (12Ã—3)      â”‚
â”‚  "it is important to note"           â”‚ âœ… Yes â”‚ +18             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  TOTAL                               â”‚        â”‚ +97             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
2ï¸âƒ£ MARKDOWN ARTIFACTS DETECTION
Code Block
JavaScript

// Located in: detectHighCertaintyPatterns(text) function

// 3. Copy-Paste Artifacts (Markdown bolding of headers/lists in short text)
// E.g. "**Conclusion**" or "**Step 1:**" or "1. Point"
if (text.length < 200) {
  if (/\*\*(.*?)\*\*/.test(text) && (text.match(/\*\*/g) || []).length >= 2) {
    reasons.push('Heavy markdown usage characteristic of AI');
  }
  // Numbered lists in short text (e.g. "1. Plan 2. Execute")
  if (/\b\d+\.\s+[A-Z]/.test(text) && (text.match(/\d+\.\s+/g) || []).length >= 2) {
    reasons.push('Numbered list in short text');
  }
}
Additional Related Code in detectAIPatterns
JavaScript

// Located in: detectAIPatterns(text) function

// Check for numbered lists in paragraphs (AI loves numbered lists)
const numberedPoints = text.match(/\b\d+\.\s+[A-Z]/g);
if (numberedPoints && numberedPoints.length >= 3) {
  pushReason('Structured list format');
  addScore(8);
}
Explanation
text

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MARKDOWN ARTIFACTS DETECTION                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WHAT IT DOES:                                                  â”‚
â”‚  Detects leftover markdown formatting from AI copy-paste        â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  PATTERNS DETECTED:                                             â”‚
â”‚                                                                 â”‚
â”‚  1. BOLD MARKDOWN (**)                                          â”‚
â”‚     Pattern: /\*\*(.*?)\*\*/                                    â”‚
â”‚     Matches: "**Conclusion**", "**Important**"                  â”‚
â”‚     Condition: At least 2 ** pairs in text < 200 chars          â”‚
â”‚                                                                 â”‚
â”‚  2. NUMBERED LISTS                                              â”‚
â”‚     Pattern: /\b\d+\.\s+[A-Z]/                                  â”‚
â”‚     Matches: "1. First point", "2. Second point"                â”‚
â”‚     Condition: At least 2 numbered items found                  â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  WHY SHORT TEXT ONLY (<200 chars)?                              â”‚
â”‚  â€¢ Long text with markdown might be intentional                 â”‚
â”‚  â€¢ Short text with markdown = likely copy-paste artifact        â”‚
â”‚  â€¢ Reduces false positives                                      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Visual Flow
text

INPUT TEXT: "**Quick Tips:**
             1. Always plan ahead
             2. Stay organized
             3. Review your work"

(Length: ~70 characters = SHORT TEXT)

DETECTION PROCESS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  CHECK 1: Is text < 200 characters?                             â”‚
â”‚           70 < 200 â†’ âœ… YES                                     â”‚
â”‚                                                                 â”‚
â”‚  CHECK 2: Bold markdown (**)?                                   â”‚
â”‚           "**Quick Tips:**" found                               â”‚
â”‚           ** count = 2 (opening + closing)                      â”‚
â”‚           2 >= 2 â†’ âœ… YES                                       â”‚
â”‚           â†’ Add reason: "Heavy markdown usage..."               â”‚
â”‚                                                                 â”‚
â”‚  CHECK 3: Numbered lists?                                       â”‚
â”‚           "1. Always" found                                     â”‚
â”‚           "2. Stay" found                                       â”‚
â”‚           "3. Review" found                                     â”‚
â”‚           Count = 3 >= 2 â†’ âœ… YES                               â”‚
â”‚           â†’ Add reason: "Numbered list in short text"           â”‚
â”‚                                                                 â”‚
â”‚  RESULT: 2 markdown artifacts detected = RED FLAG               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
3ï¸âƒ£ TERMINAL EMOJIS DETECTION
Code Block
JavaScript

// Located in: detectHighCertaintyPatterns(text) function

// 1. Terminal Emoji Patterns (e.g. "Here it is! ğŸš€")
// Check for emoji in the last 5 chars of the string
if (/[\u{1F300}-\u{1FAFF}][^a-zA-Z0-9]*$/u.test(text)) {
  reasons.push('AI-style terminal emoji');
}
Explanation
text

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TERMINAL EMOJI DETECTION                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WHAT IT DOES:                                                  â”‚
â”‚  Detects emojis placed at the END of text (AI habit)            â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  THE REGEX EXPLAINED:                                           â”‚
â”‚                                                                 â”‚
â”‚  /[\u{1F300}-\u{1FAFF}][^a-zA-Z0-9]*$/u                         â”‚
â”‚   â”‚                    â”‚             â”‚ â”‚                        â”‚
â”‚   â”‚                    â”‚             â”‚ â””â”€â”€ Unicode flag         â”‚
â”‚   â”‚                    â”‚             â””â”€â”€â”€â”€ End of string        â”‚
â”‚   â”‚                    â””â”€â”€â”€â”€ No letters/numbers after emoji     â”‚
â”‚   â””â”€â”€â”€â”€ Emoji unicode range (ğŸŒ€ to ğŸª¿)                          â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  EXAMPLES:                                                      â”‚
â”‚                                                                 â”‚
â”‚  âœ… DETECTED (Terminal Emoji):                                  â”‚
â”‚     "Hope this helps! ğŸ˜Š"                                       â”‚
â”‚     "Great job! ğŸ‰"                                             â”‚
â”‚     "Here's the answer ğŸš€"                                      â”‚
â”‚     "Thanks! âœ¨ğŸ’¡"                                               â”‚
â”‚                                                                 â”‚
â”‚  âŒ NOT DETECTED (Mid-sentence):                                â”‚
â”‚     "I ğŸ˜Š really liked it"                                      â”‚
â”‚     "The ğŸš€ launch was great today"                             â”‚
â”‚     "So ğŸ˜‚ funny omg"                                           â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  WHY THIS MATTERS:                                              â”‚
â”‚  â€¢ AI tends to place emojis at the END of messages              â”‚
â”‚  â€¢ Humans scatter emojis throughout text                        â”‚
â”‚  â€¢ Terminal emoji = AI writing signature                        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Visual Flow
text

INPUT TEXT: "Thanks for your question! I hope this helps! ğŸ˜Š"

DETECTION PROCESS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  TEXT: "Thanks for your question! I hope this helps! ğŸ˜Š"       â”‚
â”‚                                                        â†‘â†‘       â”‚
â”‚                                                   Last chars    â”‚
â”‚                                                                 â”‚
â”‚  REGEX CHECK:                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Pattern: [\u{1F300}-\u{1FAFF}][^a-zA-Z0-9]*$              â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Step 1: Find emoji at position                            â”‚  â”‚
â”‚  â”‚         ğŸ˜Š is at index (end - 1) âœ…                       â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Step 2: Check what follows emoji                          â”‚  â”‚
â”‚  â”‚         Nothing follows (end of string) âœ…                â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Step 3: Pattern matches!                                  â”‚  â”‚
â”‚  â”‚         â†’ "AI-style terminal emoji" added to reasons      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  RESULT: Terminal emoji detected = RED FLAG                     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
4ï¸âƒ£ SELF-DISCLOSURE DETECTION
Code Block (Part 1: In detectAIPatterns)
JavaScript

// Located in: detectAIPatterns(text) function
// Inside the aiPhrases array

{ 
  pattern: /\b(as an ai|as a language model|i don'?t have personal)\b/i, 
  score: 95, 
  msg: '[AI] Self-identification', 
  strong: true 
}
Code Block (Part 2: In detectHighCertaintyPatterns)
JavaScript

// Located in: detectHighCertaintyPatterns(text) function

// 2. Explicit AI Self-Disclosure
if (/\b(as an ai|as a large language model|i do not have personal)\b/i.test(text)) {
  reasons.push('Explicit AI self-disclosure');
}
Explanation
text

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SELF-DISCLOSURE DETECTION                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WHAT IT DOES:                                                  â”‚
â”‚  Detects when AI reveals its nature or limitations              â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  PATTERNS DETECTED:                                             â”‚
â”‚                                                                 â”‚
â”‚  LOCATION 1: aiPhrases array (detectAIPatterns)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Pattern: /\b(as an ai|as a language model|                â”‚  â”‚
â”‚  â”‚             i don'?t have personal)\b/i                   â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Matches:                                                  â”‚  â”‚
â”‚  â”‚ â€¢ "As an AI..."                                           â”‚  â”‚
â”‚  â”‚ â€¢ "As a language model..."                                â”‚  â”‚
â”‚  â”‚ â€¢ "I don't have personal experiences"                     â”‚  â”‚
â”‚  â”‚ â€¢ "I dont have personal feelings"                         â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Score: 95 (Very High)                                     â”‚  â”‚
â”‚  â”‚ Strong Signal: true (Definitive AI marker)                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  LOCATION 2: detectHighCertaintyPatterns (Red Flag)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Pattern: /\b(as an ai|as a large language model|          â”‚  â”‚
â”‚  â”‚             i do not have personal)\b/i                   â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Matches:                                                  â”‚  â”‚
â”‚  â”‚ â€¢ "As an AI..."                                           â”‚  â”‚
â”‚  â”‚ â€¢ "As a large language model..."                          â”‚  â”‚
â”‚  â”‚ â€¢ "I do not have personal opinions"                       â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Result: Adds to Red Flag reasons                          â”‚  â”‚
â”‚  â”‚ Used for: Short text override                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  WHY TWO LOCATIONS?                                             â”‚
â”‚  â€¢ aiPhrases: Adds score (+95) to overall calculation           â”‚
â”‚  â€¢ HighCertainty: Triggers RED FLAG override for short text     â”‚
â”‚  â€¢ Both ensure self-disclosure is never missed                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Visual Flow
text

INPUT TEXT: "As an AI language model, I don't have personal 
             feelings or experiences. However, I can help..."

DETECTION PROCESS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  STEP 1: detectAIPatterns() checks                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Pattern: /\b(as an ai|as a language model|i don'?t have...)/   â”‚
â”‚                                                                 â”‚
â”‚  "As an AI language model" â†’ âœ… MATCH                           â”‚
â”‚   â””â”€â”€ Add reason: "[AI] Self-identification"                    â”‚
â”‚   â””â”€â”€ Add score: +95                                            â”‚
â”‚   â””â”€â”€ Set hasStrongSignal = true                                â”‚
â”‚                                                                 â”‚
â”‚  "I don't have personal feelings" â†’ âœ… MATCH                    â”‚
â”‚   â””â”€â”€ Already matched (same pattern catches both)               â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  STEP 2: detectHighCertaintyPatterns() checks                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Pattern: /\b(as an ai|as a large language model|...)/          â”‚
â”‚                                                                 â”‚
â”‚  "As an AI" â†’ âœ… MATCH                                          â”‚
â”‚   â””â”€â”€ Add reason: "Explicit AI self-disclosure"                 â”‚
â”‚   â””â”€â”€ RED FLAG: detected = true                                 â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  RESULT: DEFINITIVE AI DETECTION                                â”‚
â”‚  â€¢ Score: +95                                                   â”‚
â”‚  â€¢ Strong Signal: true                                          â”‚
â”‚  â€¢ Red Flag: true                                               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
5ï¸âƒ£ TEMPLATE PHRASES DETECTION
Code Block
JavaScript

// Located in: detectHighCertaintyPatterns(text) function

// 4. Common AI Phrases (High Certainty)
// "It is important to note", "In conclusion", "Quick tips", "Brief summary"
const phrasePatterns = [
  /\b(it is|it's) (important|crucial|essential) to (note|remember|understand)\b/i,
  /\b(worth noting|notable) that\b/i,
  /\b(in conclusion|to conclude|to summarize)\b/i,
  /\b(quick (tips|summary|notes?)|brief (summary|overview))\b/i,
  /\b(hope (this|that) helps)\b/i
];

for (const pattern of phrasePatterns) {
  if (pattern.test(text)) {
    reasons.push('High-certainty AI phrase');
    break; // One is enough to trigger red flag
  }
}
Additional Template-like Patterns in aiPhrases
JavaScript

// Located in: detectAIPatterns(text) function
// Inside the aiPhrases array

// These are also template-like patterns:
{ pattern: /\b(it'?s worth noting|it'?s important to note)\b/i, score: 15, msg: '[Style] Common AI transitional phrase' },
{ pattern: /\b(in (conclusion|summary|today'?s|this))\b/i, score: 10, msg: '[Style] Formulaic transitions' },
{ pattern: /\b(it is (important|crucial|essential|vital) to (note|understand|remember))\b/i, score: 18, msg: '[Style] AI emphasis pattern' },
Explanation
text

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TEMPLATE PHRASES DETECTION                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WHAT IT DOES:                                                  â”‚
â”‚  Detects formulaic/template phrases AI repeatedly uses          â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  PHRASES DETECTED (High Certainty):                             â”‚
â”‚                                                                 â”‚
â”‚  1. EMPHASIS TEMPLATES                                          â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ Pattern: /\b(it is|it's) (important|crucial|essential)  â”‚ â”‚
â”‚     â”‚          to (note|remember|understand)\b/i              â”‚ â”‚
â”‚     â”‚                                                         â”‚ â”‚
â”‚     â”‚ Matches:                                                â”‚ â”‚
â”‚     â”‚ â€¢ "It is important to note that..."                     â”‚ â”‚
â”‚     â”‚ â€¢ "It's crucial to understand..."                       â”‚ â”‚
â”‚     â”‚ â€¢ "It is essential to remember..."                      â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  2. NOTATION TEMPLATES                                          â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ Pattern: /\b(worth noting|notable) that\b/i             â”‚ â”‚
â”‚     â”‚                                                         â”‚ â”‚
â”‚     â”‚ Matches:                                                â”‚ â”‚
â”‚     â”‚ â€¢ "It's worth noting that..."                           â”‚ â”‚
â”‚     â”‚ â€¢ "Notable that..."                                     â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  3. CONCLUSION TEMPLATES                                        â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ Pattern: /\b(in conclusion|to conclude|to summarize)\b/ â”‚ â”‚
â”‚     â”‚                                                         â”‚ â”‚
â”‚     â”‚ Matches:                                                â”‚ â”‚
â”‚     â”‚ â€¢ "In conclusion..."                                    â”‚ â”‚
â”‚     â”‚ â€¢ "To conclude..."                                      â”‚ â”‚
â”‚     â”‚ â€¢ "To summarize..."                                     â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  4. SUMMARY TEMPLATES                                           â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ Pattern: /\b(quick (tips|summary|notes?)|               â”‚ â”‚
â”‚     â”‚          brief (summary|overview))\b/i                  â”‚ â”‚
â”‚     â”‚                                                         â”‚ â”‚
â”‚     â”‚ Matches:                                                â”‚ â”‚
â”‚     â”‚ â€¢ "Quick tips:"                                         â”‚ â”‚
â”‚     â”‚ â€¢ "Quick summary"                                       â”‚ â”‚
â”‚     â”‚ â€¢ "Brief overview"                                      â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  5. CLOSING TEMPLATES                                           â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ Pattern: /\b(hope (this|that) helps)\b/i                â”‚ â”‚
â”‚     â”‚                                                         â”‚ â”‚
â”‚     â”‚ Matches:                                                â”‚ â”‚
â”‚     â”‚ â€¢ "Hope this helps!"                                    â”‚ â”‚
â”‚     â”‚ â€¢ "Hope that helps!"                                    â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  IMPORTANT: "break" after first match                           â”‚
â”‚  â€¢ Only ONE template phrase triggers red flag                   â”‚
â”‚  â€¢ Prevents over-counting                                       â”‚
â”‚  â€¢ One is enough evidence                                       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Visual Flow
text

INPUT TEXT: "It is important to note that there are several 
             factors. In conclusion, hope this helps!"

DETECTION PROCESS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  CHECK EACH PATTERN:                                            â”‚
â”‚                                                                 â”‚
â”‚  Pattern 1: /\b(it is|it's) (important|crucial|essential)       â”‚
â”‚             to (note|remember|understand)\b/i                   â”‚
â”‚                                                                 â”‚
â”‚     "It is important to note" â†’ âœ… MATCH!                       â”‚
â”‚                                                                 â”‚
â”‚     â†’ Add reason: "High-certainty AI phrase"                    â”‚
â”‚     â†’ BREAK (stop checking more patterns)                       â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  NOTE: "In conclusion" and "hope this helps" are also           â”‚
â”‚  present but NOT checked because we already found one match     â”‚
â”‚  and used "break" to exit the loop.                             â”‚
â”‚                                                                 â”‚
â”‚  This prevents the same text from getting 3 red flags           â”‚
â”‚  when 1 is enough.                                              â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  RESULT: Template phrase detected = RED FLAG                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ğŸ“Š COMPLETE SUMMARY TABLE
text

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PATTERN DETECTION SUMMARY                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                â”‚
â”‚  ELEMENT           â”‚ FUNCTION                    â”‚ SCORE     â”‚ EFFECT         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  AI Phrases        â”‚ detectAIPatterns()          â”‚ 8-95      â”‚ Adds to score  â”‚
â”‚                    â”‚ â””â”€â”€ aiPhrases array         â”‚           â”‚ + reasons      â”‚
â”‚                    â”‚                             â”‚           â”‚                â”‚
â”‚  Markdown          â”‚ detectHighCertaintyPatterns â”‚ N/A       â”‚ RED FLAG       â”‚
â”‚  Artifacts         â”‚ â””â”€â”€ Short text only (<200)  â”‚           â”‚ override       â”‚
â”‚                    â”‚                             â”‚           â”‚                â”‚
â”‚  Terminal          â”‚ detectHighCertaintyPatterns â”‚ N/A       â”‚ RED FLAG       â”‚
â”‚  Emojis            â”‚ â””â”€â”€ Emoji at end check      â”‚           â”‚ override       â”‚
â”‚                    â”‚                             â”‚           â”‚                â”‚
â”‚  Self-Disclosure   â”‚ BOTH functions              â”‚ 95        â”‚ Highest score  â”‚
â”‚                    â”‚ â”œâ”€â”€ aiPhrases (score)       â”‚           â”‚ + RED FLAG     â”‚
â”‚                    â”‚ â””â”€â”€ HighCertainty (flag)    â”‚           â”‚                â”‚
â”‚                    â”‚                             â”‚           â”‚                â”‚
â”‚  Template          â”‚ detectHighCertaintyPatterns â”‚ N/A       â”‚ RED FLAG       â”‚
â”‚  Phrases           â”‚ â””â”€â”€ phrasePatterns array    â”‚           â”‚ override       â”‚
â”‚                                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ğŸ”„ HOW THEY WORK TOGETHER
text

                            INPUT TEXT
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    detectAIPatterns(text)                     â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  AI Phrases     â”‚  â”‚ Self-Disclosure â”‚  â”‚  Structure    â”‚  â”‚
â”‚  â”‚  Detection      â”‚  â”‚  (Score: +95)   â”‚  â”‚  Patterns     â”‚  â”‚
â”‚  â”‚  (Score: 8-95)  â”‚  â”‚                 â”‚  â”‚  (Score: 8-14)â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                    â”‚                   â”‚          â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                â”‚                              â”‚
â”‚                                â–¼                              â”‚
â”‚                    RETURNS: { score, reasons,                 â”‚
â”‚                               hasStrongSignal }               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               detectHighCertaintyPatterns(text)               â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Terminal   â”‚  â”‚  Self-       â”‚  â”‚  Markdown           â”‚   â”‚
â”‚  â”‚  Emojis     â”‚  â”‚  Disclosure  â”‚  â”‚  Artifacts          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                â”‚                     â”‚              â”‚
â”‚         â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚              â”‚
â”‚         â”‚     â”‚  Template Phrases   â”‚          â”‚              â”‚
â”‚         â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚              â”‚
â”‚         â”‚                â”‚                     â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                          â”‚                                    â”‚
â”‚                          â–¼                                    â”‚
â”‚              RETURNS: { detected: true/false,                 â”‚
â”‚                         reasons: [...] }                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    scoreSegment() combines:                   â”‚
â”‚                                                               â”‚
â”‚  â€¢ Statistical score (from statisticalAnalyzer.js)            â”‚
â”‚  â€¢ Pattern score (from detectAIPatterns)                      â”‚
â”‚  â€¢ Red flags (from detectHighCertaintyPatterns)               â”‚
â”‚                                                               â”‚
â”‚  SHORT TEXT (<50 words):                                      â”‚
â”‚    If redFlags.detected â†’ Force high score (85+)              â”‚
â”‚    Else â†’ 40% statistical + 60% pattern                       â”‚
â”‚                                                               â”‚
â”‚  LONG TEXT (50+ words):                                       â”‚
â”‚    75% statistical + 25% pattern                              â”‚
â”‚                                                               â”‚
â”‚                          â–¼                                    â”‚
â”‚              FINAL SCORE (0-100)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

======================================================================================================================================================================

 OVERVIEW: Statistical Features Location
text

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STATISTICAL ANALYZER STRUCTURE               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  CORE FEATURES (You Asked About):                               â”‚
â”‚  â”œâ”€â”€ 1. Sentence Variance     â†’ analyzeSentences()              â”‚
â”‚  â”œâ”€â”€ 2. Token Entropy         â†’ calculateTokenEntropy()         â”‚
â”‚  â”œâ”€â”€ 3. N-gram Repetition     â†’ analyzeNgramRepetition()        â”‚
â”‚  â”œâ”€â”€ 4. Lexical Diversity     â†’ calculateLexicalDiversity()     â”‚
â”‚  â””â”€â”€ 5. Transition Density    â†’ calculateTransitionDensity()    â”‚
â”‚                                                                 â”‚
â”‚  ADDITIONAL FEATURES:                                           â”‚
â”‚  â”œâ”€â”€ Readability Score        â†’ calculateReadability()          â”‚
â”‚  â”œâ”€â”€ POS Regularity           â†’ analyzePOSRegularity()          â”‚
â”‚  â”œâ”€â”€ Punctuation Analysis     â†’ analyzePunctuation()            â”‚
â”‚  â”œâ”€â”€ Burstiness               â†’ calculateBurstiness()           â”‚
â”‚  â”œâ”€â”€ Stopword Ratio           â†’ calculateStopwordRatio()        â”‚
â”‚  â”œâ”€â”€ Function Word Uniformity â†’ calculateFunctionWordUniformity()â”‚
â”‚  â”œâ”€â”€ Filler Ratio             â†’ calculateFillerRatio()          â”‚
â”‚  â”œâ”€â”€ Emoji Habits             â†’ checkEmojiHabits()              â”‚
â”‚  â”œâ”€â”€ Entropy Volatility       â†’ calculateEntropyVolatility()    â”‚
â”‚  â””â”€â”€ Copy-Paste Artifacts     â†’ detectCopyPasteArtifacts()      â”‚
â”‚                                                                 â”‚
â”‚  MAIN FUNCTION:                                                 â”‚
â”‚  â””â”€â”€ analyzeTextStatistics()  â†’ Combines all features           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
1ï¸âƒ£ SENTENCE VARIANCE
Code Block
JavaScript

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// SENTENCE ANALYSIS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function analyzeSentences(text) {
  // Split into sentences (handle multiple punctuation patterns)
  const sentences = text
    .split(/[.!?]+/)
    .map(s => s.trim())
    .filter(s => s.length > 0);

  if (sentences.length === 0) return { mean: 0, variance: 0, count: 0 };

  const lengths = sentences.map(s => s.split(/\s+/).length);
  const mean = lengths.reduce((a, b) => a + b, 0) / lengths.length;

  // Calculate variance
  const variance = lengths.reduce((sum, len) => sum + Math.pow(len - mean, 2), 0) / lengths.length;

  return {
    mean,
    variance,
    stdDev: Math.sqrt(variance),
    count: sentences.length,
    minLength: Math.min(...lengths),
    maxLength: Math.max(...lengths),
    lengths
  };
}
How It's Used in Main Function
JavaScript

// In analyzeTextStatistics():
const sentences = analyzeSentences(text);

// Normalization:
const normalizedFeatures = {
  sentenceLengthVariance: normalizeFeature(sentences.variance, HUMAN_BASELINES.sentenceLengthVariance),
  sentenceLengthMean: normalizeFeature(sentences.mean, HUMAN_BASELINES.sentenceLengthMean),
  // ...
};

// Generating reasons:
if (normalizedFeatures.sentenceLengthVariance > threshold) {
  if (sentences.variance < HUMAN_BASELINES.sentenceLengthVariance.mean * 0.5) {
    reasons.push('Unusually uniform sentence lengths');
  } else {
    reasons.push('Highly variable sentence structure');
  }
}
Human Baseline
JavaScript

const HUMAN_BASELINES = {
  sentenceLengthMean: { mean: 18, stdDev: 6 },
  sentenceLengthVariance: { mean: 45, stdDev: 25 },
  // ...
};
Explanation
text

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SENTENCE VARIANCE ANALYSIS                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WHAT IT MEASURES:                                              â”‚
â”‚  How much sentence lengths DIFFER from each other               â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  WHY IT MATTERS:                                                â”‚
â”‚  â€¢ AI writes sentences of SIMILAR lengths (low variance)        â”‚
â”‚  â€¢ Humans write sentences of VARIED lengths (high variance)     â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  STEP-BY-STEP LOGIC:                                            â”‚
â”‚                                                                 â”‚
â”‚  STEP 1: Split text into sentences                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Input:  "I love coding. It is fun! Really?"               â”‚  â”‚
â”‚  â”‚ Regex:  text.split(/[.!?]+/)                              â”‚  â”‚
â”‚  â”‚ Output: ["I love coding", "It is fun", "Really"]          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  STEP 2: Count words in each sentence                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ "I love coding"  â†’ 3 words                                â”‚  â”‚
â”‚  â”‚ "It is fun"      â†’ 3 words                                â”‚  â”‚
â”‚  â”‚ "Really"         â†’ 1 word                                 â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ lengths = [3, 3, 1]                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  STEP 3: Calculate mean (average)                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Formula: sum / count                                      â”‚  â”‚
â”‚  â”‚ (3 + 3 + 1) / 3 = 7/3 = 2.33                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  STEP 4: Calculate variance                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Formula: Î£(length - mean)Â² / count                        â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ (3 - 2.33)Â² = 0.45                                        â”‚  â”‚
â”‚  â”‚ (3 - 2.33)Â² = 0.45                                        â”‚  â”‚
â”‚  â”‚ (1 - 2.33)Â² = 1.77                                        â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Variance = (0.45 + 0.45 + 1.77) / 3 = 0.89                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  STEP 5: Calculate standard deviation                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ stdDev = âˆšvariance = âˆš0.89 = 0.94                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Visual: Low vs High Variance
text

LOW VARIANCE (AI-like):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  Sentence 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (16 words)                        â”‚
â”‚  Sentence 2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  (15 words)                        â”‚
â”‚  Sentence 3: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (17 words)                       â”‚
â”‚  Sentence 4: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (16 words)                        â”‚
â”‚                                                                 â”‚
â”‚  Mean: 16 words                                                 â”‚
â”‚  Variance: ~0.5 (very low) ğŸš¨                                   â”‚
â”‚  All sentences nearly same length = AI SIGNAL                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

HIGH VARIANCE (Human-like):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  Sentence 1: â–ˆâ–ˆâ–ˆâ–ˆ (4 words)                                     â”‚
â”‚  Sentence 2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (30 words)          â”‚
â”‚  Sentence 3: â–ˆâ–ˆ (2 words)                                       â”‚
â”‚  Sentence 4: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (20 words)                    â”‚
â”‚                                                                 â”‚
â”‚  Mean: 14 words                                                 â”‚
â”‚  Variance: ~120 (very high) âœ…                                  â”‚
â”‚  Sentences vary wildly = HUMAN SIGNAL                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
What It Returns
JavaScript

{
  mean: 2.33,              // Average sentence length
  variance: 0.89,          // How spread out lengths are
  stdDev: 0.94,            // Square root of variance
  count: 3,                // Number of sentences
  minLength: 1,            // Shortest sentence
  maxLength: 3,            // Longest sentence
  lengths: [3, 3, 1]       // Array of all lengths
}
2ï¸âƒ£ TOKEN ENTROPY
Code Block
JavaScript

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// TOKEN ENTROPY (Measures predictability)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function calculateTokenEntropy(text) {
  const tokens = text.toLowerCase()
    .replace(/[^\w\s]/g, ' ')
    .split(/\s+/)
    .filter(t => t.length > 0);

  if (tokens.length === 0) return 0;

  // Count token frequencies
  const frequencies = {};
  tokens.forEach(token => {
    frequencies[token] = (frequencies[token] || 0) + 1;
  });

  // Calculate Shannon entropy
  const totalTokens = tokens.length;
  let entropy = 0;

  Object.values(frequencies).forEach(count => {
    const probability = count / totalTokens;
    entropy -= probability * Math.log2(probability);
  });

  return entropy;
}
How It's Used in Main Function
JavaScript

// In analyzeTextStatistics():
const entropy = calculateTokenEntropy(text);

// Normalization:
const normalizedFeatures = {
  tokenEntropy: normalizeFeature(entropy, baselines.tokenEntropy || HUMAN_BASELINES.tokenEntropy),
  // ...
};

// Entropy floor for short texts:
if (lexical && typeof lexical.uniqueWords === 'number' && lexical.uniqueWords < 10) {
  normalizedFeatures.tokenEntropy = 0;
}

// Generating reasons:
if (normalizedFeatures.tokenEntropy > threshold) {
  if (entropy < HUMAN_BASELINES.tokenEntropy.mean) {
    reasons.push('Predictable word patterns');
  } else {
    reasons.push('Unusually random word distribution');
  }
}
Human Baseline
JavaScript

const HUMAN_BASELINES = {
  tokenEntropy: { mean: 8.5, stdDev: 1.5 },
  // ...
};
Explanation
text

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TOKEN ENTROPY ANALYSIS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WHAT IT MEASURES:                                              â”‚
â”‚  How RANDOM or PREDICTABLE word choices are                     â”‚
â”‚  Based on Shannon's Information Theory                          â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  WHY IT MATTERS:                                                â”‚
â”‚  â€¢ AI uses PREDICTABLE word patterns (low entropy)              â”‚
â”‚  â€¢ Humans use more VARIED vocabulary (high entropy)             â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  STEP-BY-STEP LOGIC:                                            â”‚
â”‚                                                                 â”‚
â”‚  STEP 1: Tokenize text (extract words)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Input:  "The cat sat on the mat. The cat slept."          â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ .toLowerCase()                                            â”‚  â”‚
â”‚  â”‚ â†’ "the cat sat on the mat. the cat slept."                â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ .replace(/[^\w\s]/g, ' ')                                 â”‚  â”‚
â”‚  â”‚ â†’ "the cat sat on the mat  the cat slept "                â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ .split(/\s+/).filter(t => t.length > 0)                   â”‚  â”‚
â”‚  â”‚ â†’ ["the", "cat", "sat", "on", "the", "mat",               â”‚  â”‚
â”‚  â”‚    "the", "cat", "slept"]                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  STEP 2: Count frequency of each token                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ frequencies = {                                           â”‚  â”‚
â”‚  â”‚   "the":   3,                                             â”‚  â”‚
â”‚  â”‚   "cat":   2,                                             â”‚  â”‚
â”‚  â”‚   "sat":   1,                                             â”‚  â”‚
â”‚  â”‚   "on":    1,                                             â”‚  â”‚
â”‚  â”‚   "mat":   1,                                             â”‚  â”‚
â”‚  â”‚   "slept": 1                                              â”‚  â”‚
â”‚  â”‚ }                                                         â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Total tokens: 9                                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  STEP 3: Calculate probability of each token                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ probability = count / totalTokens                         â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ "the":   3/9 = 0.333                                      â”‚  â”‚
â”‚  â”‚ "cat":   2/9 = 0.222                                      â”‚  â”‚
â”‚  â”‚ "sat":   1/9 = 0.111                                      â”‚  â”‚
â”‚  â”‚ "on":    1/9 = 0.111                                      â”‚  â”‚
â”‚  â”‚ "mat":   1/9 = 0.111                                      â”‚  â”‚
â”‚  â”‚ "slept": 1/9 = 0.111                                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  STEP 4: Apply Shannon Entropy Formula                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Formula: H = -Î£(p Ã— logâ‚‚(p))                              â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ For each token:                                           â”‚  â”‚
â”‚  â”‚   entropy -= probability * Math.log2(probability)         â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ "the":   -0.333 Ã— logâ‚‚(0.333) = -0.333 Ã— (-1.58) = 0.53  â”‚  â”‚
â”‚  â”‚ "cat":   -0.222 Ã— logâ‚‚(0.222) = -0.222 Ã— (-2.17) = 0.48  â”‚  â”‚
â”‚  â”‚ "sat":   -0.111 Ã— logâ‚‚(0.111) = -0.111 Ã— (-3.17) = 0.35  â”‚  â”‚
â”‚  â”‚ "on":    -0.111 Ã— logâ‚‚(0.111) = 0.35                      â”‚  â”‚
â”‚  â”‚ "mat":   -0.111 Ã— logâ‚‚(0.111) = 0.35                      â”‚  â”‚
â”‚  â”‚ "slept": -0.111 Ã— logâ‚‚(0.111) = 0.35                      â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Total Entropy = 0.53 + 0.48 + 0.35Ã—4 = 2.41 bits          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Visual: Entropy Concept
text

ENTROPY SCALE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  0 bits â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º âˆ bits     â”‚
â”‚                                                                 â”‚
â”‚  LOW ENTROPY                                      HIGH ENTROPY  â”‚
â”‚  (Predictable)                                    (Random)      â”‚
â”‚                                                                 â”‚
â”‚     ğŸ¤– AI                                           ğŸ‘¤ Human    â”‚
â”‚                                                                 â”‚
â”‚  Example:                            Example:                   â”‚
â”‚  "the the the the"                   "cat dog bird fish"        â”‚
â”‚  (only 1 unique word)                (all unique words)         â”‚
â”‚  Entropy â‰ˆ 0 bits                    Entropy â‰ˆ 2 bits           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LOW ENTROPY TEXT (AI-like):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "The system is important. The system works well.               â”‚
â”‚   The system helps users. The system is reliable."              â”‚
â”‚                                                                 â”‚
â”‚  Token frequencies:                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  THE â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (4Ã—)                               â”‚   â”‚
â”‚  â”‚  SYSTEM â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (4Ã—)                            â”‚   â”‚
â”‚  â”‚  IS â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (2Ã—)                                        â”‚   â”‚
â”‚  â”‚  other words â–ˆ (1Ã— each)                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  Many repeated words = LOW ENTROPY = PREDICTABLE ğŸš¨             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

HIGH ENTROPY TEXT (Human-like):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "My golden retriever loves chasing squirrels through           â”‚
â”‚   the backyard during autumn mornings."                         â”‚
â”‚                                                                 â”‚
â”‚  Token frequencies:                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  my â–ˆ (1Ã—)                                               â”‚   â”‚
â”‚  â”‚  golden â–ˆ (1Ã—)                                           â”‚   â”‚
â”‚  â”‚  retriever â–ˆ (1Ã—)                                        â”‚   â”‚
â”‚  â”‚  loves â–ˆ (1Ã—)                                            â”‚   â”‚
â”‚  â”‚  chasing â–ˆ (1Ã—)                                          â”‚   â”‚
â”‚  â”‚  ... all unique!                                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  All unique words = HIGH ENTROPY = UNPREDICTABLE âœ…             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
What It Returns
JavaScript

// Returns a single number (entropy in bits)
return entropy;  // e.g., 2.41

// Higher = more random/varied (human-like)
// Lower = more predictable/repetitive (AI-like)
3ï¸âƒ£ N-GRAM REPETITION
Code Block
JavaScript

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// N-GRAM REPETITION RATE
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function analyzeNgramRepetition(text, n = 3) {
  const words = text.toLowerCase()
    .replace(/[^\w\s]/g, ' ')
    .split(/\s+/)
    .filter(w => w.length > 0);

  if (words.length < n) return { repetitionRate: 0, uniqueRatio: 1 };

  const ngrams = [];
  for (let i = 0; i <= words.length - n; i++) {
    ngrams.push(words.slice(i, i + n).join(' '));
  }

  const uniqueNgrams = new Set(ngrams);
  const repetitionRate = 1 - (uniqueNgrams.size / ngrams.length);

  // Find most repeated n-grams
  const ngramCounts = {};
  ngrams.forEach(ng => {
    ngramCounts[ng] = (ngramCounts[ng] || 0) + 1;
  });

  const repeated = Object.entries(ngramCounts)
    .filter(([_, count]) => count > 1)
    .sort((a, b) => b[1] - a[1])
    .slice(0, 5);

  return {
    repetitionRate,
    uniqueRatio: uniqueNgrams.size / ngrams.length,
    totalNgrams: ngrams.length,
    uniqueNgrams: uniqueNgrams.size,
    mostRepeated: repeated
  };
}
How It's Used in Main Function
JavaScript

// In analyzeTextStatistics():
const ngrams = analyzeNgramRepetition(text, 3);  // Using trigrams (n=3)

// Normalization:
const normalizedFeatures = {
  ngramRepetition: normalizeFeature(ngrams.repetitionRate, HUMAN_BASELINES.ngramRepetition),
  // ...
};

// Generating reasons:
if (normalizedFeatures.ngramRepetition > threshold) {
  reasons.push(`Repetitive phrasing (${(ngrams.repetitionRate * 100).toFixed(0)}% repeated)`);
}
Human Baseline
JavaScript

const HUMAN_BASELINES = {
  ngramRepetition: { mean: 0.15, stdDev: 0.1 },
  // ...
};
Explanation
text

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    N-GRAM REPETITION ANALYSIS                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WHAT IT MEASURES:                                              â”‚
â”‚  How often CONSECUTIVE WORD SEQUENCES repeat in text            â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  WHAT IS AN N-GRAM?                                             â”‚
â”‚  â€¢ N-gram = sequence of N consecutive words                     â”‚
â”‚  â€¢ Unigram (n=1): "the", "cat", "sat"                           â”‚
â”‚  â€¢ Bigram (n=2):  "the cat", "cat sat"                          â”‚
â”‚  â€¢ Trigram (n=3): "the cat sat", "cat sat on"                   â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  WHY IT MATTERS:                                                â”‚
â”‚  â€¢ AI often REPEATS similar phrase patterns                     â”‚
â”‚  â€¢ Humans naturally VARY their phrasing                         â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  STEP-BY-STEP LOGIC:                                            â”‚
â”‚                                                                 â”‚
â”‚  STEP 1: Extract words                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Input: "The cat sat on the mat. The cat slept."           â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ words = ["the", "cat", "sat", "on", "the", "mat",         â”‚  â”‚
â”‚  â”‚          "the", "cat", "slept"]                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  STEP 2: Create n-grams (n=3, trigrams)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Sliding window of 3 words:                                â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Position 0: [the, cat, sat]    â†’ "the cat sat"            â”‚  â”‚
â”‚  â”‚ Position 1: [cat, sat, on]     â†’ "cat sat on"             â”‚  â”‚
â”‚  â”‚ Position 2: [sat, on, the]     â†’ "sat on the"             â”‚  â”‚
â”‚  â”‚ Position 3: [on, the, mat]     â†’ "on the mat"             â”‚  â”‚
â”‚  â”‚ Position 4: [the, mat, the]    â†’ "the mat the"            â”‚  â”‚
â”‚  â”‚ Position 5: [mat, the, cat]    â†’ "mat the cat"            â”‚  â”‚
â”‚  â”‚ Position 6: [the, cat, slept]  â†’ "the cat slept"          â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Total: 7 trigrams                                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  STEP 3: Find unique n-grams                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ uniqueNgrams = Set of all trigrams                        â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ All 7 trigrams are unique in this example                 â”‚  â”‚
â”‚  â”‚ Unique count: 7                                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  STEP 4: Calculate repetition rate                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Formula: repetitionRate = 1 - (unique / total)            â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ repetitionRate = 1 - (7 / 7) = 1 - 1 = 0                  â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ 0% repetition = all unique = good (human-like)            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  STEP 5: Find most repeated n-grams                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Count occurrences of each n-gram                          â”‚  â”‚
â”‚  â”‚ Filter those appearing > 1 time                           â”‚  â”‚
â”‚  â”‚ Sort by frequency (descending)                            â”‚  â”‚
â”‚  â”‚ Return top 5                                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Visual: N-gram Extraction Process
text

TEXT: "I love to code and I love to learn"

STEP 1: Get words
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ["i", "love", "to", "code", "and", "i", "love", "to", "learn"] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 2: Create trigrams (n=3)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  Position 0: â”‚ i â”‚ love â”‚ to â”‚ code â”‚ and â”‚ i â”‚ love â”‚ to â”‚ learn â”‚
â”‚              â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜                                  â”‚
â”‚              "i love to"                                        â”‚
â”‚                                                                 â”‚
â”‚  Position 1: â”‚ i â”‚ love â”‚ to â”‚ code â”‚ and â”‚ i â”‚ love â”‚ to â”‚ learn â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                  "love to code"                                 â”‚
â”‚                                                                 â”‚
â”‚  Position 2: â”‚ i â”‚ love â”‚ to â”‚ code â”‚ and â”‚ i â”‚ love â”‚ to â”‚ learn â”‚
â”‚                        â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                        "to code and"                            â”‚
â”‚                                                                 â”‚
â”‚  ... and so on ...                                              â”‚
â”‚                                                                 â”‚
â”‚  Position 6: â”‚ i â”‚ love â”‚ to â”‚ code â”‚ and â”‚ i â”‚ love â”‚ to â”‚ learn â”‚
â”‚                                              â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                                              "love to learn"    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 3: Find repeats
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  "i love to"      â†’ 2 times âš ï¸ REPEATED                         â”‚
â”‚  "love to code"   â†’ 1 time                                      â”‚
â”‚  "to code and"    â†’ 1 time                                      â”‚
â”‚  "code and i"     â†’ 1 time                                      â”‚
â”‚  "and i love"     â†’ 1 time                                      â”‚
â”‚  "i love to"      â†’ (already counted)                           â”‚
â”‚  "love to learn"  â†’ 1 time                                      â”‚
â”‚                                                                 â”‚
â”‚  Total: 7 trigrams                                              â”‚
â”‚  Unique: 6 trigrams                                             â”‚
â”‚  Repetition Rate: 1 - (6/7) = 14.3%                             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Visual: High vs Low Repetition
text

HIGH REPETITION (AI-like):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "It is important to note that. It is important to remember.   â”‚
â”‚   It is important to understand that it is important."         â”‚
â”‚                                                                 â”‚
â”‚  Repeated trigrams:                                             â”‚
â”‚  â€¢ "it is important" â†’ 4 times ğŸš¨                               â”‚
â”‚  â€¢ "is important to" â†’ 4 times ğŸš¨                               â”‚
â”‚  â€¢ "important to note" â†’ 1 time                                 â”‚
â”‚  â€¢ "important to remember" â†’ 1 time                             â”‚
â”‚  â€¢ "important to understand" â†’ 1 time                           â”‚
â”‚                                                                 â”‚
â”‚  Repetition Rate: ~45% ğŸš¨ HIGH                                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LOW REPETITION (Human-like):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "My dog loves playing fetch. She runs across the yard          â”‚
â”‚   and brings back the ball every single time."                  â”‚
â”‚                                                                 â”‚
â”‚  All trigrams are unique!                                       â”‚
â”‚  â€¢ "my dog loves" â†’ 1 time                                      â”‚
â”‚  â€¢ "dog loves playing" â†’ 1 time                                 â”‚
â”‚  â€¢ "loves playing fetch" â†’ 1 time                               â”‚
â”‚  â€¢ ... all unique ...                                           â”‚
â”‚                                                                 â”‚
â”‚  Repetition Rate: ~0% âœ… LOW                                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
What It Returns
JavaScript

{
  repetitionRate: 0.143,           // 14.3% of n-grams repeated
  uniqueRatio: 0.857,              // 85.7% are unique
  totalNgrams: 7,                  // Total n-grams generated
  uniqueNgrams: 6,                 // How many are unique
  mostRepeated: [                  // Top 5 repeated n-grams
    ["i love to", 2],              // [n-gram, count]
    // ...
  ]
}
4ï¸âƒ£ LEXICAL DIVERSITY (Type-Token Ratio)
Code Block
JavaScript

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// LEXICAL DIVERSITY (Type-Token Ratio)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function calculateLexicalDiversity(text) {
  const words = text.toLowerCase()
    .replace(/[^\w\s]/g, ' ')
    .split(/\s+/)
    .filter(w => w.length > 0);

  if (words.length === 0) return { ttr: 0, uniqueWords: 0 };

  const uniqueWords = new Set(words);
  const ttr = uniqueWords.size / words.length;

  // Moving-average type-token ratio (MATTR) for longer texts
  const windowSize = Math.min(100, words.length);
  let mattr = 0;

  if (words.length >= windowSize) {
    let windows = 0;
    for (let i = 0; i <= words.length - windowSize; i += 10) {
      const window = words.slice(i, i + windowSize);
      const windowUnique = new Set(window);
      mattr += windowUnique.size / windowSize;
      windows++;
    }
    mattr /= windows;
  } else {
    mattr = ttr;
  }

  return {
    ttr,
    mattr,
    uniqueWords: uniqueWords.size,
    totalWords: words.length,
    interpretation: ttr > 0.7 ? 'High diversity' : ttr > 0.5 ? 'Moderate diversity' : 'Low diversity'
  };
}
How It's Used in Main Function
JavaScript

// In analyzeTextStatistics():
const lexical = calculateLexicalDiversity(text);

// Normalization (inverted - low diversity = AI-like):
const normalizedFeatures = {
  lexicalDiversity: normalizeFeature(1 - lexical.ttr, { mean: 0.35, stdDev: 0.15 }),
  // ...
};

// Generating reasons:
if (normalizedFeatures.lexicalDiversity > threshold) {
  reasons.push(`Limited vocabulary diversity (TTR: ${(lexical.ttr * 100).toFixed(0)}%)`);
}
Human Baseline
JavaScript

const HUMAN_BASELINES = {
  lexicalDiversity: { mean: 0.65, stdDev: 0.15 },
  // ...
};
Explanation
text

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LEXICAL DIVERSITY ANALYSIS                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WHAT IT MEASURES:                                              â”‚
â”‚  VOCABULARY RICHNESS - ratio of unique words to total words     â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  KEY TERMS:                                                     â”‚
â”‚  â€¢ Type = Unique words                                          â”‚
â”‚  â€¢ Token = All words (including repeats)                        â”‚
â”‚  â€¢ TTR = Type-Token Ratio = Unique / Total                      â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  WHY IT MATTERS:                                                â”‚
â”‚  â€¢ AI tends to REPEAT words (low diversity)                     â”‚
â”‚  â€¢ Humans use VARIED vocabulary (high diversity)                â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  STEP-BY-STEP LOGIC:                                            â”‚
â”‚                                                                 â”‚
â”‚  STEP 1: Extract all words (tokens)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Input: "The dog chased the cat. The dog was fast."        â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ words = ["the", "dog", "chased", "the", "cat",            â”‚  â”‚
â”‚  â”‚          "the", "dog", "was", "fast"]                     â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Total tokens: 9                                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  STEP 2: Find unique words (types)                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ uniqueWords = new Set(words)                              â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Set: {"the", "dog", "chased", "cat", "was", "fast"}       â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Total types: 6                                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  STEP 3: Calculate TTR                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ TTR = uniqueWords.size / words.length                     â”‚  â”‚
â”‚  â”‚ TTR = 6 / 9 = 0.67 (67%)                                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  STEP 4: Calculate MATTR (for long texts)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â€¢ Use sliding window of 100 words                         â”‚  â”‚
â”‚  â”‚ â€¢ Calculate TTR for each window                           â”‚  â”‚
â”‚  â”‚ â€¢ Average all window TTRs                                 â”‚  â”‚
â”‚  â”‚ â€¢ More stable for long texts                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Visual: TTR Calculation
text

INPUT: "The dog chased the cat. The dog was fast."

STEP 1: Count ALL words (Tokens)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  the  dog  chased  the  cat  the  dog  was  fast                â”‚
â”‚   1    2     3      4    5    6    7    8    9                  â”‚
â”‚                                                                 â”‚
â”‚  Total Tokens = 9                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 2: Count UNIQUE words (Types)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  Word      â”‚ Count â”‚ Unique?                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€                                    â”‚
â”‚  the       â”‚  3    â”‚ âœ… (count once)                            â”‚
â”‚  dog       â”‚  2    â”‚ âœ… (count once)                            â”‚
â”‚  chased    â”‚  1    â”‚ âœ…                                         â”‚
â”‚  cat       â”‚  1    â”‚ âœ…                                         â”‚
â”‚  was       â”‚  1    â”‚ âœ…                                         â”‚
â”‚  fast      â”‚  1    â”‚ âœ…                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€                                    â”‚
â”‚  TOTAL     â”‚       â”‚ 6 unique                                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 3: Calculate TTR
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  TTR = Types / Tokens                                           â”‚
â”‚  TTR = 6 / 9                                                    â”‚
â”‚  TTR = 0.67 (67%)                                               â”‚
â”‚                                                                 â”‚
â”‚  INTERPRETATION:                                                â”‚
â”‚  0.67 > 0.5 = "Moderate diversity" âœ…                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Visual: Low vs High Diversity
text

LOW DIVERSITY (AI-like):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "The system is a system that works on the system parameters.   â”‚
â”‚   The system provides system updates for the system."           â”‚
â”‚                                                                 â”‚
â”‚  Word Cloud:                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚      SYSTEM   SYSTEM   SYSTEM   SYSTEM   SYSTEM          â”‚   â”‚
â”‚  â”‚          THE  THE  THE  THE                              â”‚   â”‚
â”‚  â”‚            is  that  a  works  on  parameters            â”‚   â”‚
â”‚  â”‚              provides  updates  for                      â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  Tokens: 20 | Types: 10 | TTR: 0.50 (50%) ğŸš¨                    â”‚
â”‚  "system" dominates = LOW DIVERSITY                             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

HIGH DIVERSITY (Human-like):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "My golden retriever loves chasing squirrels through           â”‚
â”‚   the backyard during crisp autumn mornings."                   â”‚
â”‚                                                                 â”‚
â”‚  Word Cloud:                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  my  golden  retriever  loves  chasing  squirrels        â”‚   â”‚
â”‚  â”‚  through  the  backyard  during  crisp  autumn  mornings â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚           (all words same size = all unique!)            â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  Tokens: 12 | Types: 12 | TTR: 1.00 (100%) âœ…                   â”‚
â”‚  All unique words = HIGH DIVERSITY                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
What It Returns
JavaScript

{
  ttr: 0.67,                // Type-Token Ratio (0 to 1)
  mattr: 0.67,              // Moving-Average TTR (for long texts)
  uniqueWords: 6,           // Count of unique words
  totalWords: 9,            // Total word count
  interpretation: 'Moderate diversity'  // Human-readable
}

// Interpretation thresholds:
// ttr > 0.7 = 'High diversity' (human-like)
// ttr > 0.5 = 'Moderate diversity'
// ttr <= 0.5 = 'Low diversity' (AI-like)
5ï¸âƒ£ TRANSITION DENSITY
Code Block
JavaScript

// FIX: Added transition word density (transitions per sentence)
function calculateTransitionDensity(text) {
  const transitions = ['however', 'moreover', 'therefore', 'consequently', 'furthermore', 'additionally', 'thus', 'hence', 'nevertheless', 'nonetheless', 'similarly', 'meanwhile', 'in contrast', 'on the other hand'];
  const lower = text.toLowerCase();
  let count = 0;
  transitions.forEach(t => {
    const re = new RegExp('\\b' + t.replace(/\s+/g, '\\s+') + '\\b', 'gi');
    const m = lower.match(re);
    if (m) count += m.length;
  });
  const sentences = analyzeSentences(text).count || 1;
  return count / Math.max(1, sentences);
}
How It's Used in Main Function
JavaScript

// In analyzeTextStatistics():
const transitionDensity = calculateTransitionDensity(text);

// Normalization:
const normalizedFeatures = {
  transitionDensity: normalizeFeature(transitionDensity, HUMAN_BASELINES.transitionDensity),
  // ...
};

// Generating reasons:
if (normalizedFeatures.transitionDensity > threshold) {
  reasons.push('Overuse of transition phrases');
}
Human Baseline
JavaScript

const HUMAN_BASELINES = {
  transitionDensity: { mean: 0.12, stdDev: 0.06 },
  // ...
};
Explanation
text

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRANSITION DENSITY ANALYSIS                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WHAT IT MEASURES:                                              â”‚
â”‚  How many FORMAL TRANSITION WORDS per sentence                  â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  TRANSITION WORDS DETECTED:                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â€¢ however                â€¢ furthermore                    â”‚  â”‚
â”‚  â”‚ â€¢ moreover               â€¢ additionally                   â”‚  â”‚
â”‚  â”‚ â€¢ therefore              â€¢ thus                           â”‚  â”‚
â”‚  â”‚ â€¢ consequently           â€¢ hence                          â”‚  â”‚
â”‚  â”‚ â€¢ nevertheless           â€¢ nonetheless                    â”‚  â”‚
â”‚  â”‚ â€¢ similarly              â€¢ meanwhile                      â”‚  â”‚
â”‚  â”‚ â€¢ in contrast            â€¢ on the other hand              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  WHY IT MATTERS:                                                â”‚
â”‚  â€¢ AI OVERUSES formal transition words                          â”‚
â”‚  â€¢ Humans use them more SPARINGLY and naturally                 â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  STEP-BY-STEP LOGIC:                                            â”‚
â”‚                                                                 â”‚
â”‚  STEP 1: Define transition word list                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ const transitions = ['however', 'moreover', 'therefore',  â”‚  â”‚
â”‚  â”‚   'consequently', 'furthermore', 'additionally', 'thus',  â”‚  â”‚
â”‚  â”‚   'hence', 'nevertheless', 'nonetheless', 'similarly',    â”‚  â”‚
â”‚  â”‚   'meanwhile', 'in contrast', 'on the other hand'];       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  STEP 2: Count each transition word in text                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ For each transition word:                                 â”‚  â”‚
â”‚  â”‚   Create regex: /\bhowever\b/gi                           â”‚  â”‚
â”‚  â”‚   Match in text                                           â”‚  â”‚
â”‚  â”‚   Add match count to total                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  STEP 3: Count sentences                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Use analyzeSentences() to count sentences                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  STEP 4: Calculate density                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ density = transitionCount / sentenceCount                 â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Human baseline: ~0.12 (1 transition per 8 sentences)      â”‚  â”‚
â”‚  â”‚ AI tendency: ~0.5+ (1 transition per 2 sentences) ğŸš¨      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Visual: Transition Detection
text

INPUT TEXT:
"Climate change is a serious issue. However, some people disagree.
 Moreover, action is needed urgently. Furthermore, governments must act.
 Therefore, we should all contribute. Additionally, businesses play a role."

DETECTION PROCESS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  STEP 1: Find transitions                                       â”‚
â”‚                                                                 â”‚
â”‚  Sentence 1: "Climate change is a serious issue."               â”‚
â”‚              â†’ No transitions                                   â”‚
â”‚                                                                 â”‚
â”‚  Sentence 2: "[HOWEVER], some people disagree."                 â”‚
â”‚              â†’ 1 transition found âœ…                            â”‚
â”‚                                                                 â”‚
â”‚  Sentence 3: "[MOREOVER], action is needed urgently."           â”‚
â”‚              â†’ 1 transition found âœ…                            â”‚
â”‚                                                                 â”‚
â”‚  Sentence 4: "[FURTHERMORE], governments must act."             â”‚
â”‚              â†’ 1 transition found âœ…                            â”‚
â”‚                                                                 â”‚
â”‚  Sentence 5: "[THEREFORE], we should all contribute."           â”‚
â”‚              â†’ 1 transition found âœ…                            â”‚
â”‚                                                                 â”‚
â”‚  Sentence 6: "[ADDITIONALLY], businesses play a role."          â”‚
â”‚              â†’ 1 transition found âœ…                            â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  STEP 2: Calculate density                                      â”‚
â”‚                                                                 â”‚
â”‚  Transitions found: 5                                           â”‚
â”‚  Sentences: 6                                                   â”‚
â”‚                                                                 â”‚
â”‚  Density = 5 / 6 = 0.83 transitions per sentence ğŸš¨             â”‚
â”‚                                                                 â”‚
â”‚  Human baseline: 0.12                                           â”‚
â”‚  This text: 0.83 (7Ã— higher than normal!)                       â”‚
â”‚                                                                 â”‚
â”‚  RESULT: VERY HIGH transition density = AI SIGNAL               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Visual: Normal vs High Density
text

NORMAL DENSITY (Human-like):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "I went to the store today. Bought some groceries.             â”‚
â”‚   The weather was nice. Made dinner at home.                    â”‚
â”‚   Watched a movie after. It was pretty relaxing.                â”‚
â”‚   [However], I still have work tomorrow."                       â”‚
â”‚                                                                 â”‚
â”‚  Transitions: 1 (however)                                       â”‚
â”‚  Sentences: 7                                                   â”‚
â”‚  Density: 1/7 = 0.14 âœ… (close to human baseline of 0.12)       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

HIGH DENSITY (AI-like):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "The topic is important. [Moreover], it affects everyone.      â”‚
â”‚   [Furthermore], experts agree. [Additionally], research shows. â”‚
â”‚   [Consequently], we must act. [Therefore], change is needed.   â”‚
â”‚   [Nevertheless], progress is possible."                        â”‚
â”‚                                                                 â”‚
â”‚  Transitions: 6 (moreover, furthermore, additionally,           â”‚
â”‚               consequently, therefore, nevertheless)            â”‚
â”‚  Sentences: 7                                                   â”‚
â”‚  Density: 6/7 = 0.86 ğŸš¨ (7Ã— higher than normal!)                â”‚
â”‚                                                                 â”‚
â”‚  REASON: "Overuse of transition phrases"                        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
What It Returns
JavaScript

// Returns a single number (transitions per sentence)
return count / Math.max(1, sentences);  // e.g., 0.83

// Higher = more formal transitions (AI-like)
// Lower = natural flow (human-like)
// Human baseline: ~0.12
ğŸ“Š HOW ALL FEATURES COMBINE
The Main Function Flow
JavaScript

export function analyzeTextStatistics(text) {
  // 1. Extract all features
  const sentences = analyzeSentences(text);
  const entropy = calculateTokenEntropy(text);
  const ngrams = analyzeNgramRepetition(text, 3);
  const lexical = calculateLexicalDiversity(text);
  const transitionDensity = calculateTransitionDensity(text);
  // ... other features ...

  // 2. Normalize each feature to 0-1 scale
  const normalizedFeatures = {
    sentenceLengthVariance: normalizeFeature(sentences.variance, HUMAN_BASELINES.sentenceLengthVariance),
    tokenEntropy: normalizeFeature(entropy, baselines.tokenEntropy),
    ngramRepetition: normalizeFeature(ngrams.repetitionRate, HUMAN_BASELINES.ngramRepetition),
    lexicalDiversity: normalizeFeature(1 - lexical.ttr, { mean: 0.35, stdDev: 0.15 }),
    transitionDensity: normalizeFeature(transitionDensity, HUMAN_BASELINES.transitionDensity),
    // ...
  };

  // 3. Apply weights
  const weights = {
    sentenceLengthVariance: 0.11,
    ngramRepetition: 0.11,
    lexicalDiversity: 0.09,
    tokenEntropy: 0.09,
    transitionDensity: 0.05,
    // ...
  };

  // 4. Calculate weighted score
  let score = 0;
  Object.keys(normalizedFeatures).forEach(feature => {
    score += normalizedFeatures[feature] * weights[feature];
  });
  score = score * 100;  // Convert to 0-100

  // 5. Generate reasons
  // ... (based on which features exceeded threshold)

  return { score, confidence, reasons, ... };
}
Visual: Complete Analysis Flow
text

                            INPUT TEXT
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FEATURE EXTRACTION                         â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Sentence   â”‚  â”‚   Token     â”‚  â”‚    N-gram           â”‚    â”‚
â”‚  â”‚  Variance   â”‚  â”‚   Entropy   â”‚  â”‚    Repetition       â”‚    â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚                     â”‚    â”‚
â”‚  â”‚  Raw: 45.2  â”‚  â”‚  Raw: 7.8   â”‚  â”‚  Raw: 0.18          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                â”‚                    â”‚               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Lexical   â”‚  â”‚  Transition â”‚  â”‚   + Other Features  â”‚    â”‚
â”‚  â”‚  Diversity  â”‚  â”‚   Density   â”‚  â”‚                     â”‚    â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚   (burstiness,      â”‚    â”‚
â”‚  â”‚  Raw: 0.62  â”‚  â”‚  Raw: 0.45  â”‚  â”‚    stopwords, etc.) â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                â”‚                    â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                â”‚                    â”‚
          â–¼                â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NORMALIZATION (Z-Score)                    â”‚
â”‚                                                               â”‚
â”‚  For each feature:                                            â”‚
â”‚    z = (value - baseline.mean) / baseline.stdDev              â”‚
â”‚    normalized = |z| / 3                                       â”‚
â”‚    clamp to [0, 1]                                            â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ SentVar:    â”‚  â”‚ Entropy:    â”‚  â”‚ N-gram:     â”‚            â”‚
â”‚  â”‚   0.52      â”‚  â”‚   0.47      â”‚  â”‚   0.30      â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚  â”‚ LexDiv:     â”‚  â”‚ TransDens:  â”‚                             â”‚
â”‚  â”‚   0.20      â”‚  â”‚   0.55      â”‚                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WEIGHTED COMBINATION                       â”‚
â”‚                                                               â”‚
â”‚  score = Î£(normalizedFeature Ã— weight)                        â”‚
â”‚                                                               â”‚
â”‚  sentenceLengthVariance Ã— 0.11  = 0.52 Ã— 0.11 = 0.057         â”‚
â”‚  tokenEntropy Ã— 0.09            = 0.47 Ã— 0.09 = 0.042         â”‚
â”‚  ngramRepetition Ã— 0.11         = 0.30 Ã— 0.11 = 0.033         â”‚
â”‚  lexicalDiversity Ã— 0.09        = 0.20 Ã— 0.09 = 0.018         â”‚
â”‚  transitionDensity Ã— 0.05       = 0.55 Ã— 0.05 = 0.028         â”‚
â”‚  ... + other features ...                                     â”‚
â”‚                                                               â”‚
â”‚  Total = 0.45                                                 â”‚
â”‚  Final Score = 0.45 Ã— 100 = 45                                â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GENERATE REASONS                           â”‚
â”‚                                                               â”‚
â”‚  For features > 0.6 threshold:                                â”‚
â”‚  â€¢ transitionDensity (0.55) â†’ Not quite, but close            â”‚
â”‚  â€¢ sentenceLengthVariance (0.52) â†’ Not quite                  â”‚
â”‚                                                               â”‚
â”‚  reasons = []  (no features exceeded threshold)               â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FINAL OUTPUT                               â”‚
â”‚                                                               â”‚
â”‚  {                                                            â”‚
â”‚    score: 45,                                                 â”‚
â”‚    confidence: 62,                                            â”‚
â”‚    reasons: [],                                               â”‚
â”‚    interpretation: 'Uncertain - mixed signals'                â”‚
â”‚  }                                                            â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ğŸ“‹ COMPLETE SUMMARY TABLE
text

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         STATISTICAL FEATURES SUMMARY                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                â”‚
â”‚  FEATURE           â”‚ FUNCTION                  â”‚ AI SIGNAL      â”‚ WEIGHT      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Sentence Variance â”‚ analyzeSentences()        â”‚ LOW variance   â”‚ 0.11        â”‚
â”‚                    â”‚                           â”‚ (uniform)      â”‚             â”‚
â”‚                    â”‚                           â”‚                â”‚             â”‚
â”‚  Token Entropy     â”‚ calculateTokenEntropy()   â”‚ LOW entropy    â”‚ 0.09        â”‚
â”‚                    â”‚                           â”‚ (predictable)  â”‚             â”‚
â”‚                    â”‚                           â”‚                â”‚             â”‚
â”‚  N-gram Repetition â”‚ analyzeNgramRepetition()  â”‚ HIGH repetitionâ”‚ 0.11        â”‚
â”‚                    â”‚                           â”‚ (repeats)      â”‚             â”‚
â”‚                    â”‚                           â”‚                â”‚             â”‚
â”‚  Lexical Diversity â”‚ calculateLexicalDiversity â”‚ LOW diversity  â”‚ 0.09        â”‚
â”‚                    â”‚                           â”‚ (limited vocab)â”‚             â”‚
â”‚                    â”‚                           â”‚                â”‚             â”‚
â”‚  Transition Densityâ”‚ calculateTransitionDensityâ”‚ HIGH density   â”‚ 0.05        â”‚
â”‚                    â”‚                           â”‚ (overuse)      â”‚             â”‚
â”‚                                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ğŸ¯ QUICK REFERENCE: AI vs Human Signals
text

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI vs HUMAN SIGNALS                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  FEATURE              â”‚  AI SIGNAL ğŸ¤–      â”‚  HUMAN SIGNAL ğŸ‘¤   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Sentence Variance    â”‚  LOW (uniform)     â”‚  HIGH (varied)     â”‚
â”‚  Token Entropy        â”‚  LOW (predictable) â”‚  HIGH (random)     â”‚
â”‚  N-gram Repetition    â”‚  HIGH (repeats)    â”‚  LOW (unique)      â”‚
â”‚  Lexical Diversity    â”‚  LOW (limited)     â”‚  HIGH (rich)       â”‚
â”‚  Transition Density   â”‚  HIGH (overuse)    â”‚  LOW (natural)     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

=======================================================================================================================================================================
